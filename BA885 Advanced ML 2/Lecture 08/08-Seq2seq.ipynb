{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"08-Seq2seq.ipynb","provenance":[{"file_id":"https://github.com/ndoroud/BA885_Spring_2022/blob/master/Week_4/Seq2seq.ipynb","timestamp":1649852414927}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Sequence to sequence models"],"metadata":{"id":"UjpWClSFsrUa"}},{"cell_type":"code","source":["import os\n","import re\n","import string\n","import random\n","\n","from IPython import display\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn import preprocessing\n","from sklearn.utils import shuffle"],"metadata":{"id":"AXKc-Y7Js6LO","executionInfo":{"status":"ok","timestamp":1649859337652,"user_tz":240,"elapsed":4209,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Import data"],"metadata":{"id":"mxSBTPKZuf3j"}},{"cell_type":"markdown","source":["We'll use a language dataset provided by http://www.manythings.org/anki/ which contains Spanish sentences along with their English translations."],"metadata":{"id":"PQ68WF6VvGsl"}},{"cell_type":"code","source":["tf.keras.utils.get_file('spa-eng.zip',\n","                        'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n","                        cache_dir='./',\n","                        cache_subdir='datasets',\n","                        extract=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"J79Nm3Olsu5j","outputId":"4ae51677-f654-4d8b-f853-0b79f3d40d4f","executionInfo":{"status":"ok","timestamp":1649859338865,"user_tz":240,"elapsed":6,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","2646016/2638744 [==============================] - 0s 0us/step\n","2654208/2638744 [==============================] - 0s 0us/step\n"]},{"output_type":"execute_result","data":{"text/plain":["'./datasets/spa-eng.zip'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["text_file = \"datasets/spa-eng/spa.txt\"\n","with open(text_file) as text:\n","    lines = text.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","for line in lines:\n","    english, spanish = line.split(\"\\t\")\n","    spanish = \"[start] \" + spanish + \" [end]\"\n","    text_pairs.append((english, spanish))"],"metadata":{"id":"ixz3rwLFteuS","executionInfo":{"status":"ok","timestamp":1649859338991,"user_tz":240,"elapsed":129,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["text_pairs[310:315]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LbPHhPOIt7rr","outputId":"99faeffc-402d-4bb8-e3dd-c5521cd809fb","executionInfo":{"status":"ok","timestamp":1649859338991,"user_tz":240,"elapsed":1,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(\"I'm free!\", '[start] ¡Soy libre! [end]'),\n"," (\"I'm free.\", '[start] Yo soy libre. [end]'),\n"," (\"I'm full.\", '[start] Estoy lleno. [end]'),\n"," (\"I'm full.\", '[start] Estoy llena. [end]'),\n"," (\"I'm full.\", '[start] Ya me llené. [end]')]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["text_parirs = shuffle(text_pairs)\n","\n","num_train = int(0.7 * len(text_pairs))\n","num_valid = int(0.2 * len(text_pairs))\n","\n","train_pairs = text_pairs[:num_train]\n","valid_pairs = text_pairs[num_train:num_train + num_valid]\n","test_pairs = text_pairs[num_train + num_valid:]"],"metadata":{"id":"MrkJWDGHxO1j","executionInfo":{"status":"ok","timestamp":1649859340473,"user_tz":240,"elapsed":130,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Preprocessing"],"metadata":{"id":"I9ElURlDv_84"}},{"cell_type":"markdown","source":["### Standardization & vectorization"],"metadata":{"id":"3AC2hUIwwGsa"}},{"cell_type":"code","source":["strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(\n","        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n","\n","vocab_size = 15000\n","sequence_length = 20\n","\n","source_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")\n","target_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    # target sentences are longer since they start with [start].\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_english_texts = [pair[0] for pair in train_pairs]\n","train_spanish_texts = [pair[1] for pair in train_pairs]\n","source_vectorization.adapt(train_english_texts)\n","target_vectorization.adapt(train_spanish_texts)"],"metadata":{"id":"kToH53RQuEK5","executionInfo":{"status":"ok","timestamp":1649859369044,"user_tz":240,"elapsed":14098,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["batch_size = 64\n","\n","def format_dataset(eng, spa):\n","    eng = source_vectorization(eng)\n","    spa = target_vectorization(spa)\n","    return ({\n","        \"english\": eng,\n","        \"spanish\": spa[:, :-1],\n","    }, spa[:, 1:])\n","\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","train_ds = make_dataset(train_pairs)\n","valid_ds = make_dataset(valid_pairs)"],"metadata":{"id":"sn3OmUCJzXBg","executionInfo":{"status":"ok","timestamp":1649859383436,"user_tz":240,"elapsed":1077,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["for inputs, targets in train_ds.take(1):\n","    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n","    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n","    print(f\"targets.shape: {targets.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RL9Zrrew0GwL","outputId":"99534ddd-38ae-40ae-9213-4204e80b3ec0","executionInfo":{"status":"ok","timestamp":1649859390189,"user_tz":240,"elapsed":1402,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs['english'].shape: (64, 20)\n","inputs['spanish'].shape: (64, 20)\n","targets.shape: (64, 20)\n"]}]},{"cell_type":"markdown","source":["## Sequence to sequence model"],"metadata":{"id":"AmJhsJWs0OZS"}},{"cell_type":"markdown","source":["### Encoder"],"metadata":{"id":"daC7dk1e0SqS"}},{"cell_type":"code","source":["embed_dim = 256\n","latent_dim = 1024\n","\n","encoder_input = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n","x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(encoder_input)\n","encoder_output = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x)"],"metadata":{"id":"CC9DeSvF0HC_","executionInfo":{"status":"ok","timestamp":1649859410180,"user_tz":240,"elapsed":3167,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Decoder"],"metadata":{"id":"rGIFJb290y3u"}},{"cell_type":"code","source":["decoder_input = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n","x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(decoder_input)\n","decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n","x = decoder_gru(x, initial_state=encoder_output)\n","x = layers.Dropout(0.5)(x)\n","next_target = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","seq2seq_rnn = keras.Model(inputs = [encoder_input, decoder_input], \n","                          outputs = next_target)"],"metadata":{"id":"bbDtEonc00Je","executionInfo":{"status":"ok","timestamp":1649859417449,"user_tz":240,"elapsed":1477,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["seq2seq_rnn.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7oentjZCBkK","outputId":"43a2b393-c07f-44af-ede1-8edae7940ac2","executionInfo":{"status":"ok","timestamp":1649859422451,"user_tz":240,"elapsed":211,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," english (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," spanish (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, None, 256)    3840000     ['english[0][0]']                \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, None, 256)    3840000     ['spanish[0][0]']                \n","                                                                                                  \n"," bidirectional (Bidirectional)  (None, 1024)         7876608     ['embedding[0][0]']              \n","                                                                                                  \n"," gru_1 (GRU)                    (None, None, 1024)   3938304     ['embedding_1[0][0]',            \n","                                                                  'bidirectional[0][0]']          \n","                                                                                                  \n"," dropout (Dropout)              (None, None, 1024)   0           ['gru_1[0][0]']                  \n","                                                                                                  \n"," dense (Dense)                  (None, None, 15000)  15375000    ['dropout[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 34,869,912\n","Trainable params: 34,869,912\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["seq2seq_rnn.compile(\n","    optimizer=\"adam\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"])\n","seq2seq_rnn.fit(train_ds, epochs=4, validation_data=valid_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OlNknUiZ1fkm","outputId":"6a6bdbef-7b9f-44f6-d611-70c9c21b9cfa","executionInfo":{"status":"ok","timestamp":1649859936428,"user_tz":240,"elapsed":505957,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/4\n","1302/1302 [==============================] - 125s 85ms/step - loss: 1.2761 - accuracy: 0.4414 - val_loss: 1.6105 - val_accuracy: 0.4439\n","Epoch 2/4\n","1302/1302 [==============================] - 106s 82ms/step - loss: 0.7199 - accuracy: 0.6268 - val_loss: 1.3003 - val_accuracy: 0.5302\n","Epoch 3/4\n","1302/1302 [==============================] - 106s 82ms/step - loss: 0.4930 - accuracy: 0.7081 - val_loss: 1.2119 - val_accuracy: 0.5606\n","Epoch 4/4\n","1302/1302 [==============================] - 107s 82ms/step - loss: 0.3595 - accuracy: 0.7615 - val_loss: 1.2040 - val_accuracy: 0.5714\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fe6b9af3cd0>"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["spa_vocab = target_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization([decoded_sentence])\n","        next_token_predictions = seq2seq_rnn.predict(\n","            [tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(5):\n","    input_sentence = random.choice(test_eng_texts)\n","    print(\"-\")\n","    print(input_sentence)\n","    print(decode_sequence(input_sentence))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJUOWZmL7aUx","outputId":"aa0c9734-18a2-489b-babc-261cbc1c776e","executionInfo":{"status":"ok","timestamp":1649859942153,"user_tz":240,"elapsed":5734,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["-\n","I only wish there was some way I could repay you.\n","[start] solo necesito verte lo que te haya dicho [end]\n","-\n","The desire to fly in the sky like a bird inspired the invention of the airplane.\n","[start] las hojas se ponen el avión al 5 [end]\n","-\n","A person views things differently according to whether they are rich or poor.\n","[start] una persona es más o menos una persona pobre [end]\n","-\n","You shouldn't say such a thing in the presence of children.\n","[start] no deberías decir que tal gente tan ocupado [end]\n","-\n","How could you just walk out the door without saying goodbye?\n","[start] cómo puedes hacer adiós y adiós [end]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"W1CmLUXH8GG_"},"execution_count":null,"outputs":[]}]}