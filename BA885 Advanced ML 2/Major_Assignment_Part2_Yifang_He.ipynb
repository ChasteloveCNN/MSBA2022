{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Major_Assignment_Part2_Yifang_He.ipynb","provenance":[{"file_id":"https://github.com/ndoroud/BA885_Spring_2022/blob/master/Week_6/Major_Assignment_Part2_Template.ipynb","timestamp":1651768792769}],"collapsed_sections":["L4erH6yTf0L0","oyZcLtEuQ7bm","VpmrayBJRqKk"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### Student Information:\n","- Name:\n","- BU email:\n","- Collaborators:"],"metadata":{"id":"10sGyp36wzBa"}},{"cell_type":"markdown","source":["# Major Assignment Part2\n","\n","You have already built and trained a model capable of recognizing a single digit from a 1s recording. The next step for our automated phone payment system is to extend the model to recognize 16 digits in a row (I made a mistake earlier saying that there are 12 digits!). Here is what you need to do:\n","\n","1.   Extend the single-digit voice recognition model to take a 16*16000 component waveform and output 16 digits. For simplicity you can assume that each second of the input contains the recording of a single digit.\n","\n","\n","2.   *Optional*: Provide a method to convert a recording of someone saying 16 digits in a row to a 16*16000 component verctor, A,  where \n","A[16000 j,16000 (j+1)] \n","contains a recording of a single digit.\n","\n","Be sure to submit this notebook as well as the saved weights of your final model in the h5 format."],"metadata":{"id":"iKx-k3HifUSq"}},{"cell_type":"markdown","source":["## Set up - DO NOT EDIT THIS SECTION"],"metadata":{"id":"L4erH6yTf0L0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_dp4FO5wX0W","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b9ae185e-2f74-45b6-a939-e89999ff0160","executionInfo":{"status":"ok","timestamp":1652108318402,"user_tz":240,"elapsed":13543,"user":{"displayName":"Yifang He","userId":"10128726099727892889"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow_io\n","  Downloading tensorflow_io-0.25.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.4 MB)\n","\u001b[K     |████████████████████████████████| 23.4 MB 2.6 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem==0.25.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (0.25.0)\n","Installing collected packages: tensorflow-io\n","Successfully installed tensorflow-io-0.25.0\n"]}],"source":["!pip install tensorflow_io"]},{"cell_type":"code","source":["import os\n","\n","from IPython import display\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_io as tfio\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.utils import shuffle"],"metadata":{"id":"uRnR28LYwmzT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_links = {'train_data': 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz',\n","                 'test_data': 'http://download.tensorflow.org/data/speech_commands_test_set_v0.02.tar.gz'}"],"metadata":{"id":"ibKxWgX1wt2B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for key in dataset_links:\n","    tf.keras.utils.get_file(key+'.tar.gz',\n","                            dataset_links[key],\n","                            cache_dir='./',\n","                            cache_subdir='datasets/'+key,\n","                            extract=True)"],"metadata":{"id":"JTpJ6wWTwuYY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c62eea18-c36d-4b18-b569-c2b249482135","executionInfo":{"status":"ok","timestamp":1652108447652,"user_tz":240,"elapsed":120452,"user":{"displayName":"Yifang He","userId":"10128726099727892889"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n","2428928000/2428923189 [==============================] - 35s 0us/step\n","2428936192/2428923189 [==============================] - 35s 0us/step\n","Downloading data from http://download.tensorflow.org/data/speech_commands_test_set_v0.02.tar.gz\n","112566272/112563277 [==============================] - 2s 0us/step\n","112574464/112563277 [==============================] - 2s 0us/step\n"]}]},{"cell_type":"code","source":["# Audio import function with padding\n","def load_audio(filepath):\n","    \"\"\"Takes the path of a wav audio file as input and creates\n","    a numpy array of shape (16000) as output. The input file\n","    needs to sample rate=16000. The expected duration is 1s,\n","    shorter samples are padded at the end while longer samples\n","    are cropped at 1s.\"\"\"\n","    audio = tfio.audio.AudioIOTensor(filepath)\n","    audio_rate = int(audio.rate)\n","    assert audio_rate == 16000\n","    audio = audio.to_tensor().numpy().reshape((-1)) / 32767.0\n","    audio = audio.astype(dtype=\"float32\")\n","    len = audio.shape[0]\n","    # Padding\n","    if len == 16000:\n","        return audio\n","    elif len < 16000:\n","        return np.concatenate([audio, \n","                               np.zeros(shape=(16000-len),\n","                                        dtype=\"float32\")], \n","                              axis=0)\n","    else:\n","        return audio[0:16000]\n","\n","\n","# The dataset class used to feed data to our model during training and evaluation.\n","class audio_gen(keras.utils.Sequence):\n","    def __init__(self, file_paths, labels,\n","                 batch_size=32, shape=(16*16000,),\n","                 shuffle_on_epoch_end=True):\n","        # Initialization\n","        super().__init__()\n","        self.shape = shape\n","        self.batch_size = batch_size\n","        self.labels = labels\n","        self.paths = file_paths\n","        self.n_channels = 1\n","        self.n_classes = 10\n","        self.shuffle = shuffle_on_epoch_end\n","        self.on_epoch_end()\n","    \n","    def __len__(self):\n","        return int(np.floor(len(self.paths) / self.batch_size))\n","    \n","    def __getitem__(self, idx):\n","        batch_paths = self.paths[self.batch_size * idx: \n","                                 self.batch_size * (idx+1)]\n","        batch_labels = self.labels[self.batch_size * idx:\n","                                   self.batch_size * (idx+1)]\n","        batch_samples = np.zeros(shape=(0, self.shape[0]), \n","                                 dtype='float32')\n","        for paths in batch_paths:\n","            sample = np.zeros(shape=(0), dtype='float32')\n","            for path in paths:\n","                sample = np.concatenate([sample, load_audio(path)], axis=0)\n","            batch_samples = np.concatenate([batch_samples, [sample]], axis=0)\n","        return batch_samples, np.array(batch_labels, dtype='int')\n","\n","    def on_epoch_end(self):\n","        # suffle the dataset after each epoch\n","        if self.shuffle:\n","            self.paths, self.labels = shuffle(self.paths, self.labels)"],"metadata":{"id":"E9ysvj5s37--"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data_paths = []\n","for folder, labels, samples in os.walk('./datasets/train_data/'):\n","    for sample in samples:\n","        if sample[-3:] == 'wav':\n","            train_data_paths.append([folder+'/'+sample, folder[22:]])\n","\n","df = pd.DataFrame(train_data_paths, columns=['paths', 'labels'])\n","df = df.drop(df[df['labels'] =='_background_noise_'].index)\n","categories = df['labels'].unique()\n","digits_dict = {'zero':0, 'one':1, 'two':2, \n","               'three':3, 'four':4, 'five':5,\n","               'six':6, 'seven':7, 'eight':8,\n","               'nine':9}\n","digits_index = []\n","for digit in digits_dict.keys():\n","    digits_index = digits_index + list(df[df['labels']==digit].index)\n","df = df.loc[digits_index]\n","df = df.sample(frac=1)\n","df.reset_index(inplace=True)"],"metadata":{"id":"Z_-INzgUwxYC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"qHrZJK22y7Sq","executionInfo":{"status":"ok","timestamp":1652108451841,"user_tz":240,"elapsed":106,"user":{"displayName":"Yifang He","userId":"10128726099727892889"}},"outputId":"67e3addf-52aa-4dbd-8de0-4f7377f2a553"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       index                                              paths labels\n","0      98554  ./datasets/train_data/three/7c1d8533_nohash_3.wav  three\n","1      80207    ./datasets/train_data/one/a108341b_nohash_0.wav    one\n","2      82123    ./datasets/train_data/one/5769c5ab_nohash_2.wav    one\n","3      73609    ./datasets/train_data/six/fb7eb481_nohash_0.wav    six\n","4      98072  ./datasets/train_data/three/837a0f64_nohash_4.wav  three\n","...      ...                                                ...    ...\n","38903  11516   ./datasets/train_data/five/c4cfbe43_nohash_0.wav   five\n","38904  57681   ./datasets/train_data/four/e8c3c5ca_nohash_0.wav   four\n","38905  67061  ./datasets/train_data/seven/0f2442b6_nohash_0.wav  seven\n","38906  45863   ./datasets/train_data/zero/0132a06d_nohash_2.wav   zero\n","38907  46329   ./datasets/train_data/zero/54412eae_nohash_1.wav   zero\n","\n","[38908 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-d53507b5-f3fb-465c-acad-3853db4dc455\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>paths</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>98554</td>\n","      <td>./datasets/train_data/three/7c1d8533_nohash_3.wav</td>\n","      <td>three</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>80207</td>\n","      <td>./datasets/train_data/one/a108341b_nohash_0.wav</td>\n","      <td>one</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>82123</td>\n","      <td>./datasets/train_data/one/5769c5ab_nohash_2.wav</td>\n","      <td>one</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>73609</td>\n","      <td>./datasets/train_data/six/fb7eb481_nohash_0.wav</td>\n","      <td>six</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>98072</td>\n","      <td>./datasets/train_data/three/837a0f64_nohash_4.wav</td>\n","      <td>three</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>38903</th>\n","      <td>11516</td>\n","      <td>./datasets/train_data/five/c4cfbe43_nohash_0.wav</td>\n","      <td>five</td>\n","    </tr>\n","    <tr>\n","      <th>38904</th>\n","      <td>57681</td>\n","      <td>./datasets/train_data/four/e8c3c5ca_nohash_0.wav</td>\n","      <td>four</td>\n","    </tr>\n","    <tr>\n","      <th>38905</th>\n","      <td>67061</td>\n","      <td>./datasets/train_data/seven/0f2442b6_nohash_0.wav</td>\n","      <td>seven</td>\n","    </tr>\n","    <tr>\n","      <th>38906</th>\n","      <td>45863</td>\n","      <td>./datasets/train_data/zero/0132a06d_nohash_2.wav</td>\n","      <td>zero</td>\n","    </tr>\n","    <tr>\n","      <th>38907</th>\n","      <td>46329</td>\n","      <td>./datasets/train_data/zero/54412eae_nohash_1.wav</td>\n","      <td>zero</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>38908 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d53507b5-f3fb-465c-acad-3853db4dc455')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d53507b5-f3fb-465c-acad-3853db4dc455 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d53507b5-f3fb-465c-acad-3853db4dc455');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["data_shuffled = df.sample(frac = 1)\n","nums = 28000\n","train_data_paths = np.array(data_shuffled['paths'][:nums])\n","train_labels = np.array(data_shuffled['labels'][:nums])\n","test_data_paths = np.array(data_shuffled['paths'][nums:])\n","test_labels = np.array(data_shuffled['labels'][nums:])"],"metadata":{"id":"H03NOcsKyKBP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = []\n","for path in train_data_paths:\n","    train_data.append(load_audio(path))\n","\n","test_data = []\n","for path in test_data_paths:\n","    test_data.append(load_audio(path))"],"metadata":{"id":"TrQTHwh7zKt4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data_s = []\n","for i in range(len(train_data)):\n","    train_data_s.append(get_spectrogram(train_data[i]))\n","\n","train_data_s = np.array(train_data_s)\n","train_data_s.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":389},"id":"Scx_joynzCyS","executionInfo":{"status":"error","timestamp":1652108652662,"user_tz":240,"elapsed":160281,"user":{"displayName":"Yifang He","userId":"10128726099727892889"}},"outputId":"aa027955-5800-4542-a51d-b53ee4c4c4e9"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-89e519e979ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_data_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_data_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-ea7af8b3c3bb>\u001b[0m in \u001b[0;36mget_spectrogram\u001b[0;34m(audio_tensor)\u001b[0m\n\u001b[1;32m      3\u001b[0m                                   \u001b[0mnfft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                   \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                   stride=128)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmel_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_io/python/ops/audio_ops.py\u001b[0m in \u001b[0;36mspectrogram\u001b[0;34m(input, nfft, window, stride, name)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mfft_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnfft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mwindow_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhann_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mpad_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         )\n\u001b[1;32m     49\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/signal/spectral_ops.py\u001b[0m in \u001b[0;36mstft\u001b[0;34m(signals, frame_length, frame_step, fft_length, window_fn, pad_end, name)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     framed_signals = shape_ops.frame(\n\u001b[0;32m---> 83\u001b[0;31m         signals, frame_length, frame_step, pad_end=pad_end)\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# Optionally window the framed signals.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/signal/shape_ops.py\u001b[0m in \u001b[0;36mframe\u001b[0;34m(signal, frame_length, frame_step, pad_end, pad_value, axis, name)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# [[0, 0, 0, 0], [2, 2, 2, 2], [4, 4, 4, 4]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     frame_selector = array_ops.reshape(\n\u001b[0;32m--> 198\u001b[0;31m         math_ops.range(num_frames) * subframes_per_hop, [num_frames, 1])\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;31m# subframe_selector is a [num_frames, subframes_per_frame] tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m   \"\"\"\n\u001b[0;32m--> 194\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8540\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8541\u001b[0m       return reshape_eager_fallback(\n\u001b[0;32m-> 8542\u001b[0;31m           tensor, shape, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   8543\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8544\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape_eager_fallback\u001b[0;34m(tensor, shape, name, ctx)\u001b[0m\n\u001b[1;32m   8565\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tshape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8566\u001b[0m   _result = _execute.execute(b\"Reshape\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0;32m-> 8567\u001b[0;31m                              ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m   8568\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8569\u001b[0m     _execute.record_gradient(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mAn\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mon\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \"\"\"\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mdevice_name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    950\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;34m\"\"\"Returns the device name for the current thread.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# We will ignore the constraints on credit card numbers for now.\n","\n","train_paths = np.array(df['paths'])[:32000].reshape((-1,16))\n","train_labels = np.array([digits_dict[x] for x in df['labels']])[0:32000].reshape((-1,16))\n","\n","valid_paths = np.array(df['paths'])[32000:35200].reshape((-1,16))\n","valid_labels = np.array([digits_dict[x] for x in df['labels']])[32000:35200].reshape((-1,16))\n","\n","test_paths = np.array(df['paths'])[35200:38896].reshape((-1,16))\n","test_labels = np.array([digits_dict[x] for x in df['labels']])[35200:38896].reshape((-1,16))"],"metadata":{"id":"YhYIKrff6M7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_gen = audio_gen(train_paths, train_labels)\n","valid_gen = audio_gen(valid_paths, valid_labels)\n","test_gen = audio_gen(test_paths, test_labels)"],"metadata":{"id":"8pj6kvjj8MLT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["waveform = train_data[0]\n","spectrogram = get_spectrogram(waveform)\n","spectrogram.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iZ7Thekr0PRM","executionInfo":{"status":"ok","timestamp":1652108656535,"user_tz":240,"elapsed":142,"user":{"displayName":"Yifang He","userId":"10128726099727892889"}},"outputId":"c0c323f0-a3fd-40c7-d498-de8d03d92ec7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([125, 257])"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["def get_spectrogram(audio_tensor):\n","    return tfio.audio.spectrogram(audio_tensor,\n","                                  nfft=512,\n","                                  window=256,\n","                                  stride=128)\n","\n","def mel_spectrogram(audio_tensor):\n","    return tfio.audio.melscale(get_spectrogram(audio_tensor),\n","                               rate=16000,\n","                               mels=128,\n","                               fmin=0,\n","                               fmax=8000)\n","\n","\n","def dbscale_spectrogram(audio_tensor):\n","    return tfio.audio.dbscale(mel_spectrogram(audio_tensor),\n","                              top_db=80)/60.0"],"metadata":{"id":"dXhhLa_WWQf_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation metric"],"metadata":{"id":"oyZcLtEuQ7bm"}},{"cell_type":"code","source":["# The model's prediction is accurate only if the model predicts all 16\n","# digits correctly. The custom metric below can be used to evaluate the\n","# performance of the model.\n","\n","class seq_accuracy(keras.metrics.Metric):\n","\n","    def __init__(self):\n","        super(seq_accuracy, self).__init__()\n","        self.total = self.add_weight(name='total', initializer='zeros')\n","        self.count = self.add_weight(name='count', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        # Takes the product of single-digit accuracies for each 16-digit sample.\n","        accuracies = tf.reduce_prod(\n","            tf.cast(tf.equal(y_true, tf.argmax(y_pred, axis=2)), \n","                    tf.float32), axis=1)\n","        sum_a = tf.reduce_sum(accuracies)\n","        with tf.control_dependencies([sum_a]):\n","            update_t = self.total.assign_add(sum_a)\n","        num_a = tf.cast(tf.size(accuracies), self._dtype)\n","        with tf.control_dependencies([update_t]):\n","            return self.count.assign_add(num_a)\n","    \n","    def result(self):\n","        return tf.math.divide_no_nan(self.total, self.count)\n","    \n","    def reset_states(self):\n","        self.total.assign(0.)"],"metadata":{"id":"_9OrwM9uMPNy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sample inputs and outputs"],"metadata":{"id":"VpmrayBJRqKk"}},{"cell_type":"code","source":["# Sample inputs and labels in our dataset:\n","for batch in train_gen:\n","    sample_inputs = batch[0]\n","    print(sample_inputs.shape)\n","    sample_labels = batch[1]\n","    print(sample_labels.shape)\n","    break"],"metadata":{"id":"wR8T7qkYoKOa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fd4d8eb6-177c-4c32-b6f3-a843e8d44591","executionInfo":{"status":"ok","timestamp":1652108664375,"user_tz":240,"elapsed":885,"user":{"displayName":"Yifang He","userId":"10128726099727892889"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(32, 256000)\n","(32, 16)\n"]}]},{"cell_type":"code","source":["# Sample outputs:\n","predictions = model_16.predict(sample_inputs)\n","predictions.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"xmFURdQMlMDq","outputId":"a12d4645-2196-42a8-9b3c-089ca23fc15f","executionInfo":{"status":"error","timestamp":1652108665453,"user_tz":240,"elapsed":3,"user":{"displayName":"Yifang He","userId":"10128726099727892889"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-49d9b4c8b5dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sample outputs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model_16' is not defined"]}]},{"cell_type":"code","source":["# Inference:\n","np.argmax(predictions, axis=2)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":186},"id":"JTywBLbaPYcN","outputId":"e58542c6-20b1-44dd-a315-636c9940f42c","executionInfo":{"status":"error","timestamp":1652108668839,"user_tz":240,"elapsed":126,"user":{"displayName":"Yifang He","userId":"10128726099727892889"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-8837bef0b0c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Inference:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"]}]},{"cell_type":"code","source":["sample_labels[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uuporPpPpQm","outputId":"06a4cfe0-0330-46ef-9753-9e276d4b1908","executionInfo":{"status":"ok","timestamp":1652108669310,"user_tz":240,"elapsed":2,"user":{"displayName":"Yifang He","userId":"10128726099727892889"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([7, 0, 4, 1, 4, 1, 4, 7, 8, 2, 1, 2, 0, 0, 0, 6])"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["# Edit the cells below"],"metadata":{"id":"rvRx8cROtR2D"}},{"cell_type":"markdown","source":["## Import your single-digit voice recognition model\n","\n","Import your pretrained single-digit classifier. The model should take 16000-component 'waveform' vectors as input and produce a 10-component 'softmax' vector."],"metadata":{"id":"Zx7CJ16PjtdR"}},{"cell_type":"code","source":["num_classes = 10\n","inputs = keras.Input(shape=(16000,))\n","x = layers.Lambda(lambda waveform: dbscale_spectrogram(waveform))(inputs)\n","x = layers.Reshape((125, 128, 1))(x)\n","x = layers.Conv2D(32, 4, 1, activation='relu')(x)\n","x = layers.MaxPooling2D(2)(x)\n","x = layers.Conv2D(64, 4, 1, activation='relu')(x)\n","x = layers.MaxPooling2D(2)(x)\n","x = layers.Conv2D(128, 4, 1)(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.ReLU()(x)\n","x = layers.MaxPooling2D(2)(x)\n","x = layers.Conv2D(256, 4, 1, activation='relu')(x)\n","x = layers.MaxPooling2D(2)(x)\n","x = layers.Conv2D(512, 2, 1, activation='relu')(x)\n","x = layers.GlobalAveragePooling2D()(x)\n","x = layers.Dropout(0.5)(x)\n","y = layers.Dense(num_classes, activation='softmax')(x)\n","\n","model_Conv2D_35 = keras.Model(inputs = inputs, outputs = y)\n","\n","model_Conv2D_35.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXViysqZJrJq","executionInfo":{"status":"ok","timestamp":1652108679107,"user_tz":240,"elapsed":1069,"user":{"displayName":"Yifang He","userId":"10128726099727892889"}},"outputId":"9614fa53-14ae-4028-9546-a19a52aa6a39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 16000)]           0         \n","                                                                 \n"," lambda (Lambda)             (None, 125, 128)          0         \n","                                                                 \n"," reshape (Reshape)           (None, 125, 128, 1)       0         \n","                                                                 \n"," conv2d (Conv2D)             (None, 122, 125, 32)      544       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 61, 62, 32)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 58, 59, 64)        32832     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 29, 29, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 26, 26, 128)       131200    \n","                                                                 \n"," batch_normalization (BatchN  (None, 26, 26, 128)      512       \n"," ormalization)                                                   \n","                                                                 \n"," re_lu (ReLU)                (None, 26, 26, 128)       0         \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 13, 13, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 10, 10, 256)       524544    \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 5, 5, 256)        0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 4, 4, 512)         524800    \n","                                                                 \n"," global_average_pooling2d (G  (None, 512)              0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," dropout (Dropout)           (None, 512)               0         \n","                                                                 \n"," dense (Dense)               (None, 10)                5130      \n","                                                                 \n","=================================================================\n","Total params: 1,219,562\n","Trainable params: 1,219,306\n","Non-trainable params: 256\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["## Extend to 16 digits\n","\n","Extend your single-digit classifier to classify 16 digits in parallel. The expected input of the model is a 256000-component 'waveform' while the output should be a (16,10)-shaped tensor or sixteen 10-component vectors where each 10-component vector corresponds to the classification of a single digit.\n","\n","\n","---\n","\n","\n","Hint: Consider the toy-classifier below which classifies a single digit recording.\n","\n","```\n","inputs = keras.Input(shape=(16000,))\n","x = layers.Dense(400, activation = 'relu')(inputs)\n","x = layers.Dense(10, activation = 'softmax')(x)\n","classifier1 = keras.Model(inputs = inputs, outputs = x)\n","```\n","\n","To extend this to 16 parallel classifiers we can play a simple trick inspired by the object detection model we discussed in class:\n","\n","```\n","inputs = keras.Input(shape=(16*16000,))\n","x = layers.Reshape((16,16000))(inputs)\n","x = layers.Dense(400, activation = 'relu')(x)\n","x = layers.Dense(10, activation = 'softmax')(x)\n","classifier16 = keras.Model(inputs = inputs, outputs = x)\n","```\n","This is equivalent to running 16 single-digit classifiers in parallel and feeding each only a portion of the input (input[16000 j: 16000(j+1)]).\n","\n","---\n"],"metadata":{"id":"ccn87TbtkF6G"}},{"cell_type":"markdown","source":["### Your extended model"],"metadata":{"id":"VLuAaKocRDZd"}},{"cell_type":"code","source":["# 16-digit classifier structure:\n","#\n","model_16_inputs = keras.Input(shape=(16*16000,))\n","x = layers.Reshape((16, 16000))(model_16_inputs)\n","x = layers.Dense(10, activation='softmax')(x)\n","#\n","model_16 = keras.Model(inputs = model_16_inputs, outputs = x)\n","model_16.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics = ['accuracy'])\n","#"],"metadata":{"id":"_o04kuSnlqkD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use the custom metric seq_accuracy for evaluating the performance of your model.\n","metric = seq_accuracy()\n","loss = keras.losses.sparse_categorical_crossentropy\n","model_16.compile(optimizer=\"adam\", loss=loss, metrics=[metric])\n","model_16.evaluate(test_gen)"],"metadata":{"id":"l_SskyIXutoD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652108689889,"user_tz":240,"elapsed":8032,"user":{"displayName":"Yifang He","userId":"10128726099727892889"}},"outputId":"50d969c2-0100-48a7-c31f-446e8e737bec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7/7 [==============================] - 7s 665ms/step - loss: 2.3083 - seq_accuracy: 0.0000e+00\n"]},{"output_type":"execute_result","data":{"text/plain":["[2.3082897663116455, 0.0]"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["model_16.save(\"model_16.h5\")"],"metadata":{"id":"iEyXbu5FZGpF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Parsing (*Optional*)\n","\n","So far we assumed that the waveform vector was parsed such that each 16000-long segment records a single digit. This may not be the case for a real recording thus we need to preprocess the input. If you feel extra motivated you can try writing a function which implements the following:\n","\n","\n","1.   Take as an input a waveform of arbitrary length\n","2.   Locate the spoken digits in the waveform and check that there are 16 of them.\n","3.   Pad/crop the waveform such that the spoken digits are located in 1s long segments.\n","4.   Return the resulting 256000 component vector.\n","\n"],"metadata":{"id":"siqDx0QOj3Ty"}},{"cell_type":"code","source":[""],"metadata":{"id":"Dpnz1W7MUh-O"},"execution_count":null,"outputs":[]}]}