{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Major_assignment_1.ipynb","provenance":[{"file_id":"https://github.com/ndoroud/BA885_Spring_2022/blob/master/Week_2/Major_assignment_1.ipynb","timestamp":1649254214416}],"collapsed_sections":["5hw6blBgq01B","E_4RjWtrs2b3","NqSOxN9XuADP"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Major assignment (BA885)\n","\n","In this assignment you will solve a deep learning problem from scratch!\n","\n","Your company accepts payments over the phone for its services and is now in the process of automating this task. As the sole data scientist on your team, your task is to build and train a voice recognition algorithm to record the customer's credit card number. You will do this in two stages:\n","\n","\n","\n","1.   Stage 1 (Due April 15): Gather/crowdsource your own data and build a prototype algorithm that recognizes digits.\n","\n","*   Collect audio samples with the help of your team (classmates). The recordings should have the following specifications: Duration: 1s, sample rate= 16 kHz, file name and format(See example below): {label} _ hash(count_name).wav. Try to collect at least a 100 samples (10 samples per digit).\n","\n","*   Build a simple model that takes the waveform -- a 16000 component vector -- and classifies it as a digit in [0,...,9].\n","\n","You can use the \"speech commands\" dataset for the time being until your own dataset is complete but the final model should only be trained on the samples collected by the class.\n","\n","You can use this notebook as a template for this assignment. Please share your thoughts on the following questions in your submission.\n","\n","* What is the acceptable error rate for the model?\n","\n","* What is the maximum accuracy you can reach with your limited (~3000) number of samples?\n","\n","* Can you use data generation and transfer learning to improve the performance of your model?\n","\n","\n","\n","\n","2.   Stage 2 (Due May 6): TBA\n"],"metadata":{"id":"MpH_vzspZRCz"}},{"cell_type":"markdown","source":["## Sample file names:\n"],"metadata":{"id":"5hw6blBgq01B"}},{"cell_type":"markdown","source":["\n","Use the following format when annotating the files:\n","\n","{label} _ hash(count_name).wav\n","\n","Ex: 4_297d828cd9bbfab3fd6a0ad5442e232b.wav\n","\n","where label is the digit (0,...,9), count is the sample number and name is your name. Hash stands for a hash function, we will use the md5 hash function from hashlib.\n"],"metadata":{"id":"y_e4aBsEu5UI"}},{"cell_type":"code","source":["import hashlib "],"metadata":{"id":"vlZDNTJUfWDJ","executionInfo":{"status":"ok","timestamp":1649897479670,"user_tz":240,"elapsed":2,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["hashlib.md5(b'012_Nima_Doroud').hexdigest()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"4JY-vpkbfZud","outputId":"c6d91745-cd99-4e11-bb8b-8b0a79da30a8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'297d828cd9bbfab3fd6a0ad5442e232b'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["## Speech commands dataset"],"metadata":{"id":"E_4RjWtrs2b3"}},{"cell_type":"code","source":["!pip install tensorflow_io"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xg5H87PkTYqs","outputId":"bdae7255-acb3-4c61-e4a3-fa64fba2366f","executionInfo":{"status":"ok","timestamp":1649897479176,"user_tz":240,"elapsed":6073,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow_io\n","  Downloading tensorflow_io-0.24.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.4 MB)\n","\u001b[K     |████████████████████████████████| 23.4 MB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem==0.24.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (0.24.0)\n","Installing collected packages: tensorflow-io\n","Successfully installed tensorflow-io-0.24.0\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Sk1gT7A2TLsT","executionInfo":{"status":"ok","timestamp":1649897484169,"user_tz":240,"elapsed":3170,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"outputs":[],"source":["import os\n","\n","from IPython import display\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import glob\n","import shutil\n","import random\n","from collections import Counter\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_io as tfio\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras import preprocessing\n","from sklearn.utils import shuffle"]},{"cell_type":"code","source":["_ = tf.keras.utils.get_file('speech_commands.tar.gz',\n","                            'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz',\n","                            cache_dir='./',\n","                            cache_subdir='datasets',\n","                            extract=True)"],"metadata":{"id":"9oHC2ZwXTUyy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649897558842,"user_tz":240,"elapsed":70344,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}},"outputId":"3a9e06b8-4882-4c98-cc79-20bb29a6647c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n","2428928000/2428923189 [==============================] - 15s 0us/step\n","2428936192/2428923189 [==============================] - 15s 0us/step\n"]}]},{"cell_type":"code","source":["data = []\n","categories = []\n","for folder, labels, samples in os.walk('./datasets/'):\n","    if folder[11:]:\n","        categories.append(folder[11:])\n","    for sample in samples:\n","        if sample[-3:] == 'wav':\n","            data.append([folder+'/'+sample, folder[11:]])"],"metadata":{"id":"KWTw2WBqTrXo","executionInfo":{"status":"ok","timestamp":1649897684890,"user_tz":240,"elapsed":468,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["data = np.array(data)"],"metadata":{"id":"XZ_zI8nmU3J_","executionInfo":{"status":"ok","timestamp":1649897688776,"user_tz":240,"elapsed":149,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["nums_list = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']"],"metadata":{"id":"bA2SJvDmWs0d","executionInfo":{"status":"ok","timestamp":1649897690760,"user_tz":240,"elapsed":2,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["num_samples = Counter(data[:,1])\n","for num in nums_list:\n","    print(num+' {}'.format(num_samples[num]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z0aPxeq8WQU1","outputId":"cc153d60-3e61-499d-afa4-2bd859422139","executionInfo":{"status":"ok","timestamp":1649897693192,"user_tz":240,"elapsed":138,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["zero 4052\n","one 3890\n","two 3880\n","three 3727\n","four 3728\n","five 4052\n","six 3860\n","seven 3998\n","eight 3787\n","nine 3934\n"]}]},{"cell_type":"code","source":["num_samples"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C1P98VsnkRSq","executionInfo":{"status":"ok","timestamp":1649897729033,"user_tz":240,"elapsed":160,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}},"outputId":"5d675fa6-bc35-489a-8add-ef5574b135ff"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({'_background_noise_': 6,\n","         'backward': 1664,\n","         'bed': 2014,\n","         'bird': 2064,\n","         'cat': 2031,\n","         'dog': 2128,\n","         'down': 3917,\n","         'eight': 3787,\n","         'five': 4052,\n","         'follow': 1579,\n","         'forward': 1557,\n","         'four': 3728,\n","         'go': 3880,\n","         'happy': 2054,\n","         'house': 2113,\n","         'learn': 1575,\n","         'left': 3801,\n","         'marvin': 2100,\n","         'nine': 3934,\n","         'no': 3941,\n","         'off': 3745,\n","         'on': 3845,\n","         'one': 3890,\n","         'right': 3778,\n","         'seven': 3998,\n","         'sheila': 2022,\n","         'six': 3860,\n","         'stop': 3872,\n","         'three': 3727,\n","         'tree': 1759,\n","         'two': 3880,\n","         'up': 3723,\n","         'visual': 1592,\n","         'wow': 2123,\n","         'yes': 4044,\n","         'zero': 4052})"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["## Some useful functions"],"metadata":{"id":"NqSOxN9XuADP"}},{"cell_type":"code","source":["def load_audio(filepath):\n","    # Import audio file as a tensor representing the waveform.\n","    audio = tfio.audio.AudioIOTensor(filepath)\n","    audio_rate = int(audio.rate)\n","    # Assert that the sample rate is 16000kHz.\n","    assert audio_rate == 16000\n","    # Return the waveform as a 1 dimensional float32 tensor.\n","    return tf.cast(tf.squeeze(audio.to_tensor(), axis=[-1]), tf.float32) / 32767.0\n","\n","def get_spectrogram(waveform):\n","    # Zero-padding for an audio waveform with less than 16,000 samples.\n","    input_len = 16000\n","    waveform = waveform[:input_len]\n","    zero_padding = tf.zeros(\n","        [16000] - tf.shape(waveform),\n","        dtype=tf.float32)\n","    # Cast the waveform tensors' dtype to float32.\n","    waveform = tf.cast(waveform, dtype=tf.float32)\n","    # Concatenate the waveform with `zero_padding`, which ensures all audio\n","    # clips are of the same length.\n","    equal_length = tf.concat([waveform, zero_padding], 0)\n","    # Convert the waveform to a spectrogram via a STFT.\n","    spectrogram = tf.signal.stft(\n","        equal_length, frame_length=255, frame_step=128)\n","    # Obtain the magnitude of the STFT.\n","    spectrogram = tf.abs(spectrogram)\n","    # Add a `channels` dimension, so that the spectrogram can be used\n","    # as image-like input data with convolution layers (which expect\n","    # shape (`batch_size`, `height`, `width`, `channels`).\n","    spectrogram = spectrogram[..., tf.newaxis]\n","    return spectrogram"],"metadata":{"id":"Zvwr3QRacExv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Build your model"],"metadata":{"id":"bGsGy7pEu8Zd"}},{"cell_type":"code","source":[""],"metadata":{"id":"8gwZ6xhKvC-M"},"execution_count":null,"outputs":[]}]}