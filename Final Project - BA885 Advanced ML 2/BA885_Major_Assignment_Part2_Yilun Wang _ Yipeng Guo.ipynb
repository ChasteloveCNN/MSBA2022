{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BA885_Major_Assignment_Part2_Yilun Wang & Yipeng Guo.ipynb","provenance":[{"file_id":"1dgBtkXuU3IP3WlPcyVmWV-ueH_UltR-x","timestamp":1651951599758},{"file_id":"https://github.com/ndoroud/BA885_Spring_2022/blob/master/Week_6/Major_Assignment_Part2_Template.ipynb","timestamp":1651493997238}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### Student Information:\n","- Name: Yilun Wang\n","- BU email: yilun830@bu.edu\n","- Collaborators: Yipeng Guo / ypguo@bu.edu"],"metadata":{"id":"10sGyp36wzBa"}},{"cell_type":"markdown","source":["# Major Assignment Part2\n","\n","You have already built and trained a model capable of recognizing a single digit from a 1s recording. The next step for our automated phone payment system is to extend the model to recognize 16 digits in a row (I made a mistake earlier saying that there are 12 digits!). Here is what you need to do:\n","\n","1.   Extend the single-digit voice recognition model to take a 16*16000 component waveform and output 16 digits. For simplicity you can assume that each second of the input contains the recording of a single digit.\n","\n","\n","2.   *Optional*: Provide a method to convert a recording of someone saying 16 digits in a row to a 16*16000 component verctor, A,  where \n","A[16000 j,16000 (j+1)] \n","contains a recording of a single digit.\n","\n","Be sure to submit this notebook as well as the saved weights of your final model in the h5 format."],"metadata":{"id":"iKx-k3HifUSq"}},{"cell_type":"markdown","source":["## Set up - DO NOT EDIT THIS SECTION"],"metadata":{"id":"L4erH6yTf0L0"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"x_dp4FO5wX0W","colab":{"base_uri":"https://localhost:8080/"},"outputId":"473cb3ae-067e-4bd4-c7fa-6273a1b0a056","executionInfo":{"status":"ok","timestamp":1651951677244,"user_tz":240,"elapsed":5176,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow_io\n","  Downloading tensorflow_io-0.25.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.4 MB)\n","\u001b[K     |████████████████████████████████| 23.4 MB 1.1 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem==0.25.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (0.25.0)\n","Installing collected packages: tensorflow-io\n","Successfully installed tensorflow-io-0.25.0\n"]}],"source":["!pip install tensorflow_io"]},{"cell_type":"code","source":["import os\n","\n","from IPython import display\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_io as tfio\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.utils import shuffle"],"metadata":{"id":"uRnR28LYwmzT","executionInfo":{"status":"ok","timestamp":1651951680092,"user_tz":240,"elapsed":2852,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["dataset_links = {'train_data': 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz',\n","                 'test_data': 'http://download.tensorflow.org/data/speech_commands_test_set_v0.02.tar.gz'}"],"metadata":{"id":"ibKxWgX1wt2B","executionInfo":{"status":"ok","timestamp":1651951680092,"user_tz":240,"elapsed":4,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["for key in dataset_links:\n","    tf.keras.utils.get_file(key+'.tar.gz',\n","                            dataset_links[key],\n","                            cache_dir='./',\n","                            cache_subdir='datasets/'+key,\n","                            extract=True)"],"metadata":{"id":"JTpJ6wWTwuYY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fe723314-7b00-41bb-f9b4-46c73cb1bbd2","executionInfo":{"status":"ok","timestamp":1651951767634,"user_tz":240,"elapsed":87545,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n","2428928000/2428923189 [==============================] - 27s 0us/step\n","2428936192/2428923189 [==============================] - 27s 0us/step\n","Downloading data from http://download.tensorflow.org/data/speech_commands_test_set_v0.02.tar.gz\n","112566272/112563277 [==============================] - 1s 0us/step\n","112574464/112563277 [==============================] - 1s 0us/step\n"]}]},{"cell_type":"code","source":["train_data_paths = []\n","for folder, labels, samples in os.walk('./datasets/train_data/'):\n","    for sample in samples:\n","        if sample[-3:] == 'wav':\n","            train_data_paths.append([folder+'/'+sample, folder[22:]])\n","\n","df = pd.DataFrame(train_data_paths, columns=['paths', 'labels'])\n","df = df.drop(df[df['labels'] =='_background_noise_'].index)\n","categories = df['labels'].unique()\n","digits_dict = {'zero':0, 'one':1, 'two':2, \n","               'three':3, 'four':4, 'five':5,\n","               'six':6, 'seven':7, 'eight':8,\n","               'nine':9}\n","digits_index = []\n","for digit in digits_dict.keys():\n","    digits_index = digits_index + list(df[df['labels']==digit].index)\n","df = df.loc[digits_index]\n","df = df.sample(frac=1)\n","df.reset_index(inplace=True)"],"metadata":{"id":"Z_-INzgUwxYC","executionInfo":{"status":"ok","timestamp":1651951768171,"user_tz":240,"elapsed":542,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Audio import function with padding\n","def load_audio(filepath):\n","    \"\"\"Takes the path of a wav audio file as input and creates\n","    a numpy array of shape (16000) as output. The input file\n","    needs to sample rate=16000. The expected duration is 1s,\n","    shorter samples are padded at the end while longer samples\n","    are cropped at 1s.\"\"\"\n","    audio = tfio.audio.AudioIOTensor(filepath)\n","    audio_rate = int(audio.rate)\n","    assert audio_rate == 16000\n","    audio = audio.to_tensor().numpy().reshape((-1)) / 32767.0\n","    audio = audio.astype(dtype=\"float32\")\n","    len = audio.shape[0]\n","    # Padding\n","    if len == 16000:\n","        return audio\n","    elif len < 16000:\n","        return np.concatenate([audio, \n","                               np.zeros(shape=(16000-len),\n","                                        dtype=\"float32\")], \n","                              axis=0)\n","    else:\n","        return audio[0:16000]\n","\n","\n","# The dataset class used to feed data to our model during training and evaluation.\n","class audio_gen(keras.utils.Sequence):\n","    def __init__(self, file_paths, labels,\n","                 batch_size=32, shape=(16*16000,),\n","                 shuffle_on_epoch_end=True):\n","        # Initialization\n","        super().__init__()\n","        self.shape = shape\n","        self.batch_size = batch_size\n","        self.labels = labels\n","        self.paths = file_paths\n","        self.n_channels = 1\n","        self.n_classes = 10\n","        self.shuffle = shuffle_on_epoch_end\n","        self.on_epoch_end()\n","    \n","    def __len__(self):\n","        return int(np.floor(len(self.paths) / self.batch_size))\n","    \n","    def __getitem__(self, idx):\n","        batch_paths = self.paths[self.batch_size * idx: \n","                                 self.batch_size * (idx+1)]\n","        batch_labels = self.labels[self.batch_size * idx:\n","                                   self.batch_size * (idx+1)]\n","        batch_samples = np.zeros(shape=(0, self.shape[0]), \n","                                 dtype='float32')\n","        for paths in batch_paths:\n","            sample = np.zeros(shape=(0), dtype='float32')\n","            for path in paths:\n","                sample = np.concatenate([sample, load_audio(path)], axis=0)\n","            batch_samples = np.concatenate([batch_samples, [sample]], axis=0)\n","        return batch_samples, np.array(batch_labels, dtype='int')\n","\n","    def on_epoch_end(self):\n","        # suffle the dataset after each epoch\n","        if self.shuffle:\n","            self.paths, self.labels = shuffle(self.paths, self.labels)"],"metadata":{"id":"E9ysvj5s37--","executionInfo":{"status":"ok","timestamp":1651951768171,"user_tz":240,"elapsed":2,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# We will ignore the constraints on credit card numbers for now.\n","\n","train_paths = np.array(df['paths'])[:32000].reshape((-1,16))\n","train_labels = np.array([digits_dict[x] for x in df['labels']])[0:32000].reshape((-1,16))\n","\n","valid_paths = np.array(df['paths'])[32000:35200].reshape((-1,16))\n","valid_labels = np.array([digits_dict[x] for x in df['labels']])[32000:35200].reshape((-1,16))\n","\n","test_paths = np.array(df['paths'])[35200:38896].reshape((-1,16))\n","test_labels = np.array([digits_dict[x] for x in df['labels']])[35200:38896].reshape((-1,16))"],"metadata":{"id":"YhYIKrff6M7B","executionInfo":{"status":"ok","timestamp":1651951768172,"user_tz":240,"elapsed":3,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["train_gen = audio_gen(train_paths, train_labels)\n","valid_gen = audio_gen(valid_paths, valid_labels)\n","test_gen = audio_gen(test_paths, test_labels)"],"metadata":{"id":"8pj6kvjj8MLT","executionInfo":{"status":"ok","timestamp":1651951768172,"user_tz":240,"elapsed":2,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"0ts3GecaTJ9z","executionInfo":{"status":"ok","timestamp":1651951768172,"user_tz":240,"elapsed":2,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def get_spectrogram(audio_tensor):\n","    return tfio.audio.spectrogram(audio_tensor,\n","                                  nfft=512,\n","                                  window=256,\n","                                  stride=128)\n","\n","def mel_spectrogram(audio_tensor):\n","    return tfio.audio.melscale(get_spectrogram(audio_tensor),\n","                               rate=16000,\n","                               mels=128,\n","                               fmin=0,\n","                               fmax=8000)\n","\n","\n","def dbscale_spectrogram(audio_tensor):\n","    return tfio.audio.dbscale(mel_spectrogram(audio_tensor),\n","                              top_db=80)/60.0"],"metadata":{"id":"dXhhLa_WWQf_","executionInfo":{"status":"ok","timestamp":1651951768172,"user_tz":240,"elapsed":2,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation metric"],"metadata":{"id":"oyZcLtEuQ7bm"}},{"cell_type":"code","source":["# The model's prediction is accurate only if the model predicts all 16\n","# digits correctly. The custom metric below can be used to evaluate the\n","# performance of the model.\n","\n","class seq_accuracy(keras.metrics.Metric):\n","\n","    def __init__(self):\n","        super(seq_accuracy, self).__init__()\n","        self.total = self.add_weight(name='total', initializer='zeros')\n","        self.count = self.add_weight(name='count', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        # Takes the product of single-digit accuracies for each 16-digit sample.\n","        accuracies = tf.reduce_prod(\n","            tf.cast(tf.equal(y_true, tf.argmax(y_pred, axis=2)), \n","                    tf.float32), axis=1)\n","        sum_a = tf.reduce_sum(accuracies)\n","        with tf.control_dependencies([sum_a]):\n","            update_t = self.total.assign_add(sum_a)\n","        num_a = tf.cast(tf.size(accuracies), self._dtype)\n","        with tf.control_dependencies([update_t]):\n","            return self.count.assign_add(num_a)\n","    \n","    def result(self):\n","        return tf.math.divide_no_nan(self.total, self.count)\n","    \n","    def reset_states(self):\n","        self.total.assign(0.)"],"metadata":{"id":"_9OrwM9uMPNy","executionInfo":{"status":"ok","timestamp":1651951768333,"user_tz":240,"elapsed":2,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### Sample inputs and outputs"],"metadata":{"id":"VpmrayBJRqKk"}},{"cell_type":"code","source":["# Sample inputs and labels in our dataset:\n","for batch in train_gen:\n","    sample_inputs = batch[0]\n","    print(sample_inputs.shape)\n","    sample_labels = batch[1]\n","    print(sample_labels.shape)\n","    break"],"metadata":{"id":"wR8T7qkYoKOa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"29a39d78-ff3e-4f34-c188-f12ce11f543c","executionInfo":{"status":"ok","timestamp":1651951771698,"user_tz":240,"elapsed":3366,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["(32, 256000)\n","(32, 16)\n"]}]},{"cell_type":"code","source":["# Sample outputs:\n","# predictions = model_16.predict(sample_inputs)\n","# predictions.shape"],"metadata":{"id":"xmFURdQMlMDq","executionInfo":{"status":"ok","timestamp":1651951771698,"user_tz":240,"elapsed":6,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Inference:\n","#np.argmax(predictions, axis=2)[0]"],"metadata":{"id":"JTywBLbaPYcN","executionInfo":{"status":"ok","timestamp":1651951771699,"user_tz":240,"elapsed":5,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["#sample_labels[0]"],"metadata":{"id":"5uuporPpPpQm","executionInfo":{"status":"ok","timestamp":1651951771699,"user_tz":240,"elapsed":5,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Edit the cells below"],"metadata":{"id":"rvRx8cROtR2D"}},{"cell_type":"markdown","source":["## Import your single-digit voice recognition model\n","\n","Import your pretrained single-digit classifier. The model should take 16000-component 'waveform' vectors as input and produce a 10-component 'softmax' vector."],"metadata":{"id":"Zx7CJ16PjtdR"}},{"cell_type":"code","source":["# Single-digit classifier structure:\n","#\n","#model_1_inputs = keras.Input(shape=(16000,))\n","#x = layers.Dense(10, activation='softmax')(model_1_inputs)\n","#\n","#model_1 = keras.Model(inputs = model_1_inputs, outputs = x)\n","#model_1.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n","#\n","#model_1.load_weights('./model_1.h5')\n","\n","\n","model_1_inputs = keras.Input(shape=(16000,))\n","x = layers.Lambda(lambda waveform: dbscale_spectrogram(waveform))(model_1_inputs)\n","x = layers.Reshape((125, 128, 1))(x)\n","x = layers.Conv2D(32, 4, 1, activation='relu')(x)\n","x = layers.MaxPooling2D(2)(x)\n","x = layers.Conv2D(64, 4, 1, activation='relu')(x)\n","x = layers.MaxPooling2D(2)(x)\n","x = layers.Conv2D(128, 4, 1)(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.ReLU()(x)\n","x = layers.MaxPooling2D(2)(x)\n","x = layers.Conv2D(256, 4, 1, activation='relu')(x)\n","x = layers.MaxPooling2D(2)(x)\n","x = layers.Conv2D(512, 2, 1, activation='relu')(x)\n","x = layers.GlobalAveragePooling2D()(x)\n","x = layers.Dropout(0.5)(x)\n","x = layers.Dense(128, activation='relu')(x)\n","x = layers.Dense(10, activation='softmax')(x)\n","\n","model_1 = keras.Model(inputs = model_1_inputs, outputs = x)\n","\n","model_1.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n","\n","#model_Conv2D_10 = keras.Model(inputs = inputs, outputs = z)\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"R7AURqCvkLpG","executionInfo":{"status":"ok","timestamp":1651951772308,"user_tz":240,"elapsed":613,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["We will provide pre-trained model in qtools when submitting our homework."],"metadata":{"id":"fxTW_l7f8AC0"}},{"cell_type":"code","source":["model_1.load_weights('/content/Conv2D_10_val975.h5')"],"metadata":{"id":"5fGMEIiy5ZOD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extend to 16 digits\n","\n","Extend your single-digit classifier to classify 16 digits in parallel. The expected input of the model is a 256000-component 'waveform' while the output should be a (16,10)-shaped tensor or sixteen 10-component vectors where each 10-component vector corresponds to the classification of a single digit.\n","\n","\n","---\n","\n","\n","Hint: Consider the toy-classifier below which classifies a single digit recording.\n","\n","```\n","inputs = keras.Input(shape=(16000,))\n","x = layers.Dense(400, activation = 'relu')(inputs)\n","x = layers.Dense(10, activation = 'softmax')(x)\n","classifier1 = keras.Model(inputs = inputs, outputs = x)\n","```\n","\n","To extend this to 16 parallel classifiers we can play a simple trick inspired by the object detection model we discussed in class:\n","\n","```\n","inputs = keras.Input(shape=(16*16000,))\n","x = layers.Reshape((16,16000))(inputs)\n","x = layers.Dense(400, activation = 'relu')(x)\n","x = layers.Dense(10, activation = 'softmax')(x)\n","classifier16 = keras.Model(inputs = inputs, outputs = x)\n","```\n","This is equivalent to running 16 single-digit classifiers in parallel and feeding each only a portion of the input (input[16000 j: 16000(j+1)]).\n","\n","---\n"],"metadata":{"id":"ccn87TbtkF6G"}},{"cell_type":"markdown","source":["### Your extended model"],"metadata":{"id":"VLuAaKocRDZd"}},{"cell_type":"code","source":["model_16_inputs = keras.Input(shape=(16*16000,))\n","x = layers.Reshape((16,16000))(model_16_inputs)\n","x = layers.Lambda(lambda waveform: dbscale_spectrogram(waveform))(x)\n","x = layers.Reshape((2000, 128, 1))(x)\n","x = layers.Conv2D(32, 4, 1, activation='relu')(x)\n","x = layers.MaxPooling2D(2)(x)\n","x = layers.Conv2D(64, 4, 1, activation='relu')(x)\n","x = layers.MaxPooling2D(2)(x)\n","x = layers.Conv2D(128, 4, 1)(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.ReLU()(x)\n","x = layers.MaxPooling2D(2)(x)\n","x = layers.Conv2D(256, 4, 1, activation='relu')(x)\n","x = layers.MaxPooling2D(2)(x)\n","x = layers.Conv2D(512, 2, 1, activation='relu')(x)\n","#x = layers.GlobalAveragePooling2D()(x)\n","x = layers.Reshape((16,15488))(x)\n","x = layers.Dropout(0.5)(x)\n","x = layers.Dense(1024, activation='relu')(x)\n","x = layers.Dense(512, activation='relu')(x)\n","x = layers.Dense(256, activation='relu')(x)\n","x = layers.Dense(128, activation='relu')(x)\n","x = layers.Dense(10, activation='softmax')(x)\n","\n","model_16 = keras.Model(inputs = model_16_inputs, outputs = x)\n","model_16.summary()\n","\n","\n","metric = seq_accuracy()\n","loss = keras.losses.sparse_categorical_crossentropy\n","model_16.compile(optimizer=\"adam\", loss=loss, metrics = [metric])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"55rMm-QM1xm8","executionInfo":{"status":"ok","timestamp":1651951845459,"user_tz":240,"elapsed":681,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}},"outputId":"f9e59635-3355-45e5-ccfa-40aba264745a"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 256000)]          0         \n","                                                                 \n"," reshape_3 (Reshape)         (None, 16, 16000)         0         \n","                                                                 \n"," lambda_2 (Lambda)           (None, 16, 125, 128)      0         \n","                                                                 \n"," reshape_4 (Reshape)         (None, 2000, 128, 1)      0         \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 1997, 125, 32)     544       \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 998, 62, 32)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 995, 59, 64)       32832     \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 497, 29, 64)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 494, 26, 128)      131200    \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 494, 26, 128)     512       \n"," hNormalization)                                                 \n","                                                                 \n"," re_lu_1 (ReLU)              (None, 494, 26, 128)      0         \n","                                                                 \n"," max_pooling2d_6 (MaxPooling  (None, 247, 13, 128)     0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 244, 10, 256)      524544    \n","                                                                 \n"," max_pooling2d_7 (MaxPooling  (None, 122, 5, 256)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 121, 4, 512)       524800    \n","                                                                 \n"," reshape_5 (Reshape)         (None, 16, 15488)         0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 16, 15488)         0         \n","                                                                 \n"," dense_2 (Dense)             (None, 16, 1024)          15860736  \n","                                                                 \n"," dense_3 (Dense)             (None, 16, 512)           524800    \n","                                                                 \n"," dense_4 (Dense)             (None, 16, 256)           131328    \n","                                                                 \n"," dense_5 (Dense)             (None, 16, 128)           32896     \n","                                                                 \n"," dense_6 (Dense)             (None, 16, 10)            1290      \n","                                                                 \n","=================================================================\n","Total params: 17,765,482\n","Trainable params: 17,765,226\n","Non-trainable params: 256\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["callback = keras.callbacks.ModelCheckpoint(\"/content/model_16.h5\",\n","                                           monitor='val_loss',\n","                                           save_weights_only=True,\n","                                           save_best_only=True)"],"metadata":{"id":"m4pn1DIc_pRS","executionInfo":{"status":"ok","timestamp":1651951858401,"user_tz":240,"elapsed":140,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["history = model_16.fit(train_gen, validation_data=valid_gen,\n","                              epochs=100, batch_size=256, callbacks=callback)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2VCbfg_NnPP","executionInfo":{"status":"ok","timestamp":1651954552966,"user_tz":240,"elapsed":2686824,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}},"outputId":"fac5e17b-06ea-4ce8-9686-601d5075b9f8"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","62/62 [==============================] - ETA: 0s - loss: 2.1681 - seq_accuracy: 0.0000e+00"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric seq_accuracy implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n","  m.reset_state()\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r62/62 [==============================] - 44s 486ms/step - loss: 2.1681 - seq_accuracy: 0.0000e+00 - val_loss: 2.5656 - val_seq_accuracy: 0.0000e+00\n","Epoch 2/100\n","62/62 [==============================] - 28s 454ms/step - loss: 1.2361 - seq_accuracy: 9.6154e-04 - val_loss: 2.0870 - val_seq_accuracy: 0.0000e+00\n","Epoch 3/100\n","62/62 [==============================] - 29s 460ms/step - loss: 0.4985 - seq_accuracy: 0.0175 - val_loss: 1.4321 - val_seq_accuracy: 0.0000e+00\n","Epoch 4/100\n","62/62 [==============================] - 29s 467ms/step - loss: 0.3053 - seq_accuracy: 0.0434 - val_loss: 1.0404 - val_seq_accuracy: 0.0018\n","Epoch 5/100\n","62/62 [==============================] - 29s 460ms/step - loss: 0.2086 - seq_accuracy: 0.0614 - val_loss: 0.7783 - val_seq_accuracy: 0.0021\n","Epoch 6/100\n","62/62 [==============================] - 28s 450ms/step - loss: 0.1615 - seq_accuracy: 0.0645 - val_loss: 0.4366 - val_seq_accuracy: 0.0044\n","Epoch 7/100\n","62/62 [==============================] - 27s 432ms/step - loss: 0.1293 - seq_accuracy: 0.0663 - val_loss: 0.2819 - val_seq_accuracy: 0.0045\n","Epoch 8/100\n","62/62 [==============================] - 27s 430ms/step - loss: 0.1011 - seq_accuracy: 0.0671 - val_loss: 0.2429 - val_seq_accuracy: 0.0034\n","Epoch 9/100\n","62/62 [==============================] - 27s 438ms/step - loss: 0.0851 - seq_accuracy: 0.0646 - val_loss: 0.1906 - val_seq_accuracy: 0.0036\n","Epoch 10/100\n","62/62 [==============================] - 28s 442ms/step - loss: 0.0696 - seq_accuracy: 0.0642 - val_loss: 0.1861 - val_seq_accuracy: 0.0033\n","Epoch 11/100\n","62/62 [==============================] - 26s 419ms/step - loss: 0.0654 - seq_accuracy: 0.0594 - val_loss: 0.2900 - val_seq_accuracy: 0.0020\n","Epoch 12/100\n","62/62 [==============================] - 26s 420ms/step - loss: 0.0694 - seq_accuracy: 0.0532 - val_loss: 0.2943 - val_seq_accuracy: 0.0020\n","Epoch 13/100\n","62/62 [==============================] - 26s 424ms/step - loss: 0.0527 - seq_accuracy: 0.0532 - val_loss: 0.2445 - val_seq_accuracy: 0.0025\n","Epoch 14/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0446 - seq_accuracy: 0.0526 - val_loss: 0.1901 - val_seq_accuracy: 0.0027\n","Epoch 15/100\n","62/62 [==============================] - 27s 429ms/step - loss: 0.0411 - seq_accuracy: 0.0495 - val_loss: 0.2464 - val_seq_accuracy: 0.0020\n","Epoch 16/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0518 - seq_accuracy: 0.0447 - val_loss: 0.1860 - val_seq_accuracy: 0.0027\n","Epoch 17/100\n","62/62 [==============================] - 27s 425ms/step - loss: 0.0388 - seq_accuracy: 0.0448 - val_loss: 0.1964 - val_seq_accuracy: 0.0026\n","Epoch 18/100\n","62/62 [==============================] - 27s 425ms/step - loss: 0.0219 - seq_accuracy: 0.0458 - val_loss: 0.1879 - val_seq_accuracy: 0.0024\n","Epoch 19/100\n","62/62 [==============================] - 26s 425ms/step - loss: 0.0328 - seq_accuracy: 0.0415 - val_loss: 0.3796 - val_seq_accuracy: 0.0012\n","Epoch 20/100\n","62/62 [==============================] - 26s 423ms/step - loss: 0.0208 - seq_accuracy: 0.0410 - val_loss: 0.1870 - val_seq_accuracy: 0.0020\n","Epoch 21/100\n","62/62 [==============================] - 27s 433ms/step - loss: 0.0280 - seq_accuracy: 0.0383 - val_loss: 0.1631 - val_seq_accuracy: 0.0020\n","Epoch 22/100\n","62/62 [==============================] - 27s 429ms/step - loss: 0.0224 - seq_accuracy: 0.0379 - val_loss: 0.1939 - val_seq_accuracy: 0.0018\n","Epoch 23/100\n","62/62 [==============================] - 27s 430ms/step - loss: 0.0205 - seq_accuracy: 0.0357 - val_loss: 0.2223 - val_seq_accuracy: 0.0016\n","Epoch 24/100\n","62/62 [==============================] - 27s 430ms/step - loss: 0.0204 - seq_accuracy: 0.0346 - val_loss: 0.2214 - val_seq_accuracy: 0.0016\n","Epoch 25/100\n","62/62 [==============================] - 26s 423ms/step - loss: 0.0248 - seq_accuracy: 0.0323 - val_loss: 0.2406 - val_seq_accuracy: 0.0014\n","Epoch 26/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0183 - seq_accuracy: 0.0320 - val_loss: 0.2003 - val_seq_accuracy: 0.0016\n","Epoch 27/100\n","62/62 [==============================] - 26s 424ms/step - loss: 0.0255 - seq_accuracy: 0.0299 - val_loss: 0.2581 - val_seq_accuracy: 9.8720e-04\n","Epoch 28/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0193 - seq_accuracy: 0.0298 - val_loss: 0.1881 - val_seq_accuracy: 0.0015\n","Epoch 29/100\n","62/62 [==============================] - 27s 429ms/step - loss: 0.0202 - seq_accuracy: 0.0286 - val_loss: 0.2949 - val_seq_accuracy: 0.0011\n","Epoch 30/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0111 - seq_accuracy: 0.0289 - val_loss: 0.1924 - val_seq_accuracy: 0.0015\n","Epoch 31/100\n","62/62 [==============================] - 27s 430ms/step - loss: 0.0119 - seq_accuracy: 0.0278 - val_loss: 0.3201 - val_seq_accuracy: 8.3017e-04\n","Epoch 32/100\n","62/62 [==============================] - 27s 429ms/step - loss: 0.0229 - seq_accuracy: 0.0254 - val_loss: 0.2227 - val_seq_accuracy: 0.0013\n","Epoch 33/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0153 - seq_accuracy: 0.0259 - val_loss: 0.1950 - val_seq_accuracy: 0.0014\n","Epoch 34/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0190 - seq_accuracy: 0.0247 - val_loss: 0.2198 - val_seq_accuracy: 0.0012\n","Epoch 35/100\n","62/62 [==============================] - 26s 425ms/step - loss: 0.0183 - seq_accuracy: 0.0240 - val_loss: 0.1674 - val_seq_accuracy: 0.0012\n","Epoch 36/100\n","62/62 [==============================] - 27s 425ms/step - loss: 0.0225 - seq_accuracy: 0.0230 - val_loss: 0.2309 - val_seq_accuracy: 8.4252e-04\n","Epoch 37/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0130 - seq_accuracy: 0.0233 - val_loss: 0.2140 - val_seq_accuracy: 0.0011\n","Epoch 38/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0180 - seq_accuracy: 0.0223 - val_loss: 0.2629 - val_seq_accuracy: 8.5865e-04\n","Epoch 39/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0139 - seq_accuracy: 0.0219 - val_loss: 0.2230 - val_seq_accuracy: 8.8377e-04\n","Epoch 40/100\n","62/62 [==============================] - 26s 425ms/step - loss: 0.0153 - seq_accuracy: 0.0213 - val_loss: 0.2598 - val_seq_accuracy: 8.5018e-04\n","Epoch 41/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0108 - seq_accuracy: 0.0213 - val_loss: 0.2000 - val_seq_accuracy: 0.0011\n","Epoch 42/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0140 - seq_accuracy: 0.0203 - val_loss: 0.2379 - val_seq_accuracy: 8.8629e-04\n","Epoch 43/100\n","62/62 [==============================] - 26s 424ms/step - loss: 0.0110 - seq_accuracy: 0.0200 - val_loss: 0.1931 - val_seq_accuracy: 9.9393e-04\n","Epoch 44/100\n","62/62 [==============================] - 27s 432ms/step - loss: 0.0084 - seq_accuracy: 0.0199 - val_loss: 0.2636 - val_seq_accuracy: 8.4601e-04\n","Epoch 45/100\n","62/62 [==============================] - 27s 428ms/step - loss: 0.0118 - seq_accuracy: 0.0193 - val_loss: 0.2640 - val_seq_accuracy: 7.9657e-04\n","Epoch 46/100\n","62/62 [==============================] - 27s 428ms/step - loss: 0.0094 - seq_accuracy: 0.0190 - val_loss: 0.1855 - val_seq_accuracy: 9.5908e-04\n","Epoch 47/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0144 - seq_accuracy: 0.0182 - val_loss: 0.2831 - val_seq_accuracy: 6.3556e-04\n","Epoch 48/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0121 - seq_accuracy: 0.0180 - val_loss: 0.2702 - val_seq_accuracy: 8.8082e-04\n","Epoch 49/100\n","62/62 [==============================] - 27s 430ms/step - loss: 0.0108 - seq_accuracy: 0.0176 - val_loss: 0.2644 - val_seq_accuracy: 7.3154e-04\n","Epoch 50/100\n","62/62 [==============================] - 26s 424ms/step - loss: 0.0147 - seq_accuracy: 0.0171 - val_loss: 0.1913 - val_seq_accuracy: 8.8235e-04\n","Epoch 51/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0120 - seq_accuracy: 0.0170 - val_loss: 0.2031 - val_seq_accuracy: 9.4615e-04\n","Epoch 52/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0127 - seq_accuracy: 0.0164 - val_loss: 0.2294 - val_seq_accuracy: 8.3074e-04\n","Epoch 53/100\n","62/62 [==============================] - 26s 424ms/step - loss: 0.0130 - seq_accuracy: 0.0163 - val_loss: 0.1906 - val_seq_accuracy: 8.6709e-04\n","Epoch 54/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0066 - seq_accuracy: 0.0164 - val_loss: 0.2856 - val_seq_accuracy: 6.4679e-04\n","Epoch 55/100\n","62/62 [==============================] - 26s 424ms/step - loss: 0.0151 - seq_accuracy: 0.0156 - val_loss: 0.2294 - val_seq_accuracy: 7.6036e-04\n","Epoch 56/100\n","62/62 [==============================] - 26s 425ms/step - loss: 0.0130 - seq_accuracy: 0.0155 - val_loss: 0.2686 - val_seq_accuracy: 6.4010e-04\n","Epoch 57/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0076 - seq_accuracy: 0.0155 - val_loss: 0.2388 - val_seq_accuracy: 7.7399e-04\n","Epoch 58/100\n","62/62 [==============================] - 26s 422ms/step - loss: 0.0119 - seq_accuracy: 0.0148 - val_loss: 0.1797 - val_seq_accuracy: 8.0819e-04\n","Epoch 59/100\n","62/62 [==============================] - 26s 423ms/step - loss: 0.0108 - seq_accuracy: 0.0147 - val_loss: 0.1662 - val_seq_accuracy: 8.2565e-04\n","Epoch 60/100\n","62/62 [==============================] - 26s 423ms/step - loss: 0.0104 - seq_accuracy: 0.0145 - val_loss: 0.1872 - val_seq_accuracy: 7.6593e-04\n","Epoch 61/100\n","62/62 [==============================] - 26s 419ms/step - loss: 0.0095 - seq_accuracy: 0.0144 - val_loss: 0.2155 - val_seq_accuracy: 7.0817e-04\n","Epoch 62/100\n","62/62 [==============================] - 26s 417ms/step - loss: 0.0115 - seq_accuracy: 0.0140 - val_loss: 0.1861 - val_seq_accuracy: 6.5228e-04\n","Epoch 63/100\n","62/62 [==============================] - 26s 418ms/step - loss: 0.0097 - seq_accuracy: 0.0139 - val_loss: 0.2603 - val_seq_accuracy: 5.5439e-04\n","Epoch 64/100\n","62/62 [==============================] - 26s 420ms/step - loss: 0.0123 - seq_accuracy: 0.0136 - val_loss: 0.2113 - val_seq_accuracy: 6.1753e-04\n","Epoch 65/100\n","62/62 [==============================] - 26s 420ms/step - loss: 0.0147 - seq_accuracy: 0.0133 - val_loss: 0.1867 - val_seq_accuracy: 6.2217e-04\n","Epoch 66/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0138 - seq_accuracy: 0.0130 - val_loss: 0.1920 - val_seq_accuracy: 6.3363e-04\n","Epoch 67/100\n","62/62 [==============================] - 26s 425ms/step - loss: 0.0094 - seq_accuracy: 0.0130 - val_loss: 0.2496 - val_seq_accuracy: 5.8302e-04\n","Epoch 68/100\n","62/62 [==============================] - 26s 424ms/step - loss: 0.0072 - seq_accuracy: 0.0130 - val_loss: 0.2286 - val_seq_accuracy: 6.7582e-04\n","Epoch 69/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0058 - seq_accuracy: 0.0129 - val_loss: 0.2014 - val_seq_accuracy: 5.9942e-04\n","Epoch 70/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0107 - seq_accuracy: 0.0125 - val_loss: 0.1673 - val_seq_accuracy: 6.3025e-04\n","Epoch 71/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0072 - seq_accuracy: 0.0124 - val_loss: 0.2139 - val_seq_accuracy: 6.0843e-04\n","Epoch 72/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0095 - seq_accuracy: 0.0121 - val_loss: 0.2008 - val_seq_accuracy: 6.6381e-04\n","Epoch 73/100\n","62/62 [==============================] - 26s 425ms/step - loss: 0.0076 - seq_accuracy: 0.0121 - val_loss: 0.3128 - val_seq_accuracy: 4.7844e-04\n","Epoch 74/100\n","62/62 [==============================] - 27s 428ms/step - loss: 0.0088 - seq_accuracy: 0.0118 - val_loss: 0.2245 - val_seq_accuracy: 5.8997e-04\n","Epoch 75/100\n","62/62 [==============================] - 26s 424ms/step - loss: 0.0109 - seq_accuracy: 0.0117 - val_loss: 0.2434 - val_seq_accuracy: 4.3505e-04\n","Epoch 76/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0093 - seq_accuracy: 0.0115 - val_loss: 0.2557 - val_seq_accuracy: 5.2003e-04\n","Epoch 77/100\n","62/62 [==============================] - 27s 431ms/step - loss: 0.0067 - seq_accuracy: 0.0115 - val_loss: 0.2152 - val_seq_accuracy: 5.3715e-04\n","Epoch 78/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0080 - seq_accuracy: 0.0113 - val_loss: 0.2340 - val_seq_accuracy: 4.8313e-04\n","Epoch 79/100\n","62/62 [==============================] - 26s 424ms/step - loss: 0.0088 - seq_accuracy: 0.0111 - val_loss: 0.1804 - val_seq_accuracy: 5.4100e-04\n","Epoch 80/100\n","62/62 [==============================] - 27s 430ms/step - loss: 0.0059 - seq_accuracy: 0.0111 - val_loss: 0.3782 - val_seq_accuracy: 3.9637e-04\n","Epoch 81/100\n","62/62 [==============================] - 27s 428ms/step - loss: 0.0126 - seq_accuracy: 0.0107 - val_loss: 0.1804 - val_seq_accuracy: 5.1629e-04\n","Epoch 82/100\n","62/62 [==============================] - 27s 429ms/step - loss: 0.0064 - seq_accuracy: 0.0108 - val_loss: 0.2728 - val_seq_accuracy: 4.7077e-04\n","Epoch 83/100\n","62/62 [==============================] - 27s 427ms/step - loss: 0.0094 - seq_accuracy: 0.0106 - val_loss: 0.1848 - val_seq_accuracy: 5.6476e-04\n","Epoch 84/100\n","62/62 [==============================] - 26s 423ms/step - loss: 0.0067 - seq_accuracy: 0.0106 - val_loss: 0.2141 - val_seq_accuracy: 5.6351e-04\n","Epoch 85/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0119 - seq_accuracy: 0.0102 - val_loss: 0.2028 - val_seq_accuracy: 4.1090e-04\n","Epoch 86/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0040 - seq_accuracy: 0.0104 - val_loss: 0.2293 - val_seq_accuracy: 5.5575e-04\n","Epoch 87/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0075 - seq_accuracy: 0.0101 - val_loss: 0.2117 - val_seq_accuracy: 5.2823e-04\n","Epoch 88/100\n","62/62 [==============================] - 27s 426ms/step - loss: 0.0107 - seq_accuracy: 0.0099 - val_loss: 0.2552 - val_seq_accuracy: 4.2823e-04\n","Epoch 89/100\n","62/62 [==============================] - 26s 424ms/step - loss: 0.0084 - seq_accuracy: 0.0099 - val_loss: 0.1870 - val_seq_accuracy: 5.1636e-04\n","Epoch 90/100\n","62/62 [==============================] - 26s 421ms/step - loss: 0.0081 - seq_accuracy: 0.0098 - val_loss: 0.2352 - val_seq_accuracy: 4.1871e-04\n","Epoch 91/100\n","62/62 [==============================] - 26s 420ms/step - loss: 0.0089 - seq_accuracy: 0.0097 - val_loss: 0.1956 - val_seq_accuracy: 5.2016e-04\n","Epoch 92/100\n","62/62 [==============================] - 26s 422ms/step - loss: 0.0074 - seq_accuracy: 0.0096 - val_loss: 0.2763 - val_seq_accuracy: 4.0461e-04\n","Epoch 93/100\n","62/62 [==============================] - 26s 417ms/step - loss: 0.0086 - seq_accuracy: 0.0095 - val_loss: 0.1897 - val_seq_accuracy: 4.5462e-04\n","Epoch 94/100\n","62/62 [==============================] - 27s 429ms/step - loss: 0.0105 - seq_accuracy: 0.0093 - val_loss: 0.2396 - val_seq_accuracy: 4.3023e-04\n","Epoch 95/100\n","62/62 [==============================] - 27s 425ms/step - loss: 0.0078 - seq_accuracy: 0.0093 - val_loss: 0.1908 - val_seq_accuracy: 4.6923e-04\n","Epoch 96/100\n","62/62 [==============================] - 27s 440ms/step - loss: 0.0051 - seq_accuracy: 0.0093 - val_loss: 0.2162 - val_seq_accuracy: 5.1222e-04\n","Epoch 97/100\n","62/62 [==============================] - 27s 436ms/step - loss: 0.0115 - seq_accuracy: 0.0090 - val_loss: 0.2026 - val_seq_accuracy: 4.7377e-04\n","Epoch 98/100\n","62/62 [==============================] - 27s 429ms/step - loss: 0.0113 - seq_accuracy: 0.0090 - val_loss: 0.2154 - val_seq_accuracy: 4.4549e-04\n","Epoch 99/100\n","62/62 [==============================] - 27s 429ms/step - loss: 0.0103 - seq_accuracy: 0.0088 - val_loss: 0.3180 - val_seq_accuracy: 3.2030e-04\n","Epoch 100/100\n","62/62 [==============================] - 26s 423ms/step - loss: 0.0064 - seq_accuracy: 0.0089 - val_loss: 0.2420 - val_seq_accuracy: 4.4118e-04\n"]}]},{"cell_type":"code","source":["# 16-digit classifier structure:\n","#\n","# model_16_inputs = keras.Input(shape=(16*16000,))\n","# x = layers.Reshape((16, 16000))(model_16_inputs)\n","# x = layers.Dense(10, activation='softmax')(x)\n","#\n","# model_16 = keras.Model(inputs = model_16_inputs, outputs = x)\n","# model_16.compile(optimizer=\"adam\", loss=loss, metrics = [metric])\n","#"],"metadata":{"id":"_o04kuSnlqkD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use the custom metric seq_accuracy for evaluating the performance of your model.\n","metric = seq_accuracy()\n","loss = keras.losses.sparse_categorical_crossentropy\n","model_16.compile(optimizer=\"adam\", loss=loss, metrics=[metric])\n","model_16.evaluate(test_gen)"],"metadata":{"id":"l_SskyIXutoD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651945293138,"user_tz":240,"elapsed":3694,"user":{"displayName":"Yilun Wang","userId":"12667714364550598412"}},"outputId":"0f5fd76f-d384-4bd2-8c74-d96d9a8f464d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7/7 [==============================] - 4s 417ms/step - loss: 0.2453 - seq_accuracy_17: 0.4866\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.245279923081398, 0.4866071343421936]"]},"metadata":{},"execution_count":100}]},{"cell_type":"markdown","source":["In the end, we got 48.66% accuracy, which is not bad because we are predicting 16 digits at once. "],"metadata":{"id":"chp9U3p656BC"}},{"cell_type":"markdown","source":["## Parsing (*Optional*)\n","\n","So far we assumed that the waveform vector was parsed such that each 16000-long segment records a single digit. This may not be the case for a real recording thus we need to preprocess the input. If you feel extra motivated you can try writing a function which implements the following:\n","\n","\n","1.   Take as an input a waveform of arbitrary length\n","2.   Locate the spoken digits in the waveform and check that there are 16 of them.\n","3.   Pad/crop the waveform such that the spoken digits are located in 1s long segments.\n","4.   Return the resulting 256000 component vector.\n","\n"],"metadata":{"id":"siqDx0QOj3Ty"}},{"cell_type":"code","source":[""],"metadata":{"id":"Dpnz1W7MUh-O"},"execution_count":null,"outputs":[]}]}